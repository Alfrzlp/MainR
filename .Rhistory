sum(preds != rf_pred$predictions)
rf_model <-  ranger::ranger(
Y ~ ., data = df_train %>% select(-ID),
num.trees = 1100, mtry = 13,
seed = 0, max.depth = 70,
num.threads = 4,
min.node.size = 1
)
rf_pred <- predict(rf_model, df_test)
# 256
sum(preds != rf_pred$predictions)
rf_model <-  ranger::ranger(
Y ~ ., data = df_train %>% select(-ID),
num.trees = 1100, mtry = 10,
seed = 0, max.depth = 70,
num.threads = 4,
min.node.size = 1
)
rf_pred <- predict(rf_model, df_test)
# 256
sum(preds != rf_pred$predictions)
rf_model <-  ranger::ranger(
Y ~ ., data = df_train %>% select(-ID),
num.trees = 1100, mtry = 11,
seed = 1, max.depth = 70,
num.threads = 4,
min.node.size = 1
)
rf_pred <- predict(rf_model, df_test)
# 256
sum(preds != rf_pred$predictions)
rf_model <-  ranger::ranger(
Y ~ ., data = df_train %>% select(-ID),
num.trees = 1300, mtry = 11,
seed = 0, max.depth = 70,
num.threads = 4,
min.node.size = 1
)
rf_pred <- predict(rf_model, df_test)
# 256
sum(preds != rf_pred$predictions)
rf_model <-  ranger::ranger(
Y ~ ., data = df_train %>% select(-ID),
num.trees = 700, mtry = 11,
seed = 0, max.depth = 70,
num.threads = 4,
min.node.size = 1
)
rf_pred <- predict(rf_model, df_test)
# 256
sum(preds != rf_pred$predictions)
rf_model <-  ranger::ranger(
Y ~ ., data = df_train %>% select(-ID),
num.trees = 1100, mtry = 11,
seed = 0, max.depth = 70,
num.threads = 4,
min.node.size = 1
)
rf_pred <- predict(rf_model, df_test)
# 256
sum(preds != rf_pred$predictions)
df_train
my_grid <- tibble(
mtry = c(5, 11, 16),
min_n = c(5, 3, 2),
ntree = c(700, 900, 1100)
)
library(doParallel)
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
# grid search
set.seed(1)
cross(my_grid)
library(tidyverse)
library(tidymodels)
library(themis)
tidymodels_prefer()
my_grid <- tibble(
mtry = c(5, 16, 11),
min_n = c(5, 3, 2),
ntree = c(700, 900, 1100)
)
cross(my_grid)
cross_df(my_grid)
my_grid <- tibble(
mtry = c(11, 16),
min_n = c(5, 2),
ntree = c(700, 900, 1100)
)
cross_df(my_grid)
my_grid <- tibble(
mtry = c(11, 16),
min_n = c(5, 2),
ntree = c(700, 1100)
)
cross_df(my_grid)
# grid search
set.seed(1)
my_recipe %>%
summary() %>%
as.data.frame()
my_workflow <-
workflow() %>%
add_model(rf_spec) %>%
add_recipe(my_recipe)
rf_spec <-
rand_forest(mtry = tune(), min_n = 2, trees = tune()) %>%
set_engine('ranger') %>%
set_mode('classification')
# recipe and workflow -----------------------------------------------------
my_recipe <-
recipe(Y ~ ., data = train_set) %>%
# update_role(ID, new_role = "id") %>%
# step_dummy(all_nominal_predictors()) %>%
step_zv()
my_recipe %>%
summary() %>%
as.data.frame()
# recipe and workflow -----------------------------------------------------
my_recipe <-
recipe(Y ~ ., data = train_set) %>%
update_role(ID, new_role = "id") %>%
# step_dummy(all_nominal_predictors()) %>%
step_zv()
my_recipe %>%
summary() %>%
as.data.frame()
my_workflow <-
workflow() %>%
add_model(rf_spec) %>%
add_recipe(my_recipe)
# Tuning ------------------------------------------------------------------
my_grid <- tibble(
mtry = c(11, 16),
min_n = c(5, 2),
ntree = c(700, 1100)
)
cross_df(my_grid)
library(doParallel)
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
# grid search
set.seed(1)
model_res <-
my_workflow %>%
tune_grid(
resamples = train_fold,
grid = cross(my_grid),
control = control_grid(
save_pred = TRUE,
verbose = T,
allow_par = T
),
metrics = my_metric
)
cross(my_grid)
model_res <-
my_workflow %>%
tune_grid(
resamples = train_fold,
grid = cross_df(my_grid),
control = control_grid(
save_pred = TRUE,
verbose = T,
allow_par = T
),
metrics = my_metric
)
rf_spec <-
rand_forest(mtry = tune(), min_n = tune(), trees = tune()) %>%
set_engine('ranger') %>%
set_mode('classification')
my_workflow <-
workflow() %>%
add_model(rf_spec) %>%
add_recipe(my_recipe)
# grid search
set.seed(1)
model_res <-
my_workflow %>%
tune_grid(
resamples = train_fold,
grid = cross_df(my_grid),
control = control_grid(
save_pred = TRUE,
verbose = T,
allow_par = T
),
metrics = my_metric
)
rf_spec <-
rand_forest() %>%
set_engine('ranger') %>%
set_mode('classification')
# Tuning ------------------------------------------------------------------
my_grid <- tibble(
mtry = c(11, 16),
min_n = c(5, 2),
ntree = c(700, 1100)
)
cross_df(my_grid)
# grid search
set.seed(1)
model_res <-
my_workflow %>%
tune_grid(
resamples = train_fold,
grid = cross_df(my_grid),
control = control_grid(
save_pred = TRUE,
verbose = T,
allow_par = T
),
metrics = my_metric
)
rf_spec <-
rand_forest(mtry = tune(), min_n = tune(), trees = tune()) %>%
set_engine('ranger') %>%
set_mode('classification')
my_workflow <-
workflow() %>%
add_model(rf_spec) %>%
add_recipe(my_recipe)
# grid search
set.seed(1)
model_res <-
my_workflow %>%
tune_grid(
resamples = train_fold,
grid = cross_df(my_grid),
control = control_grid(
save_pred = TRUE,
verbose = T,
allow_par = T
),
metrics = my_metric
)
rf_spec <-
rand_forest(mtry = tune(), min_n = tune(), trees = tune()) %>%
set_engine('ranger') %>%
set_mode('classification')
my_workflow <-
workflow() %>%
add_model(rf_spec) %>%
add_recipe(my_recipe)
# Tuning ------------------------------------------------------------------
my_grid <- tibble(
mtry = c(11, 16),
min_n = c(5, 2),
trees = c(700, 1100)
)
cross_df(my_grid)
library(doParallel)
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
# grid search
set.seed(1)
model_res <-
my_workflow %>%
tune_grid(
resamples = train_fold,
grid = cross_df(my_grid),
control = control_grid(
save_pred = TRUE,
verbose = T,
allow_par = T
),
metrics = my_metric
)
# hasil -------------------------------------------------------------------
model_res %>%
collect_metrics() %>%
arrange(desc(mean))
best_model <- model_res %>%
select_best()
final_wf <- my_workflow %>%
finalize_workflow(best_model)
final_wf
# fit terakhir
final_fit <- final_wf %>% fit(df_train)
# -------------------------------------------------------------------------
df_train %>%
bind_cols(predict(final_fit, .)) %>%
my_metric(truth = Y, estimate = .pred_class)
df_train %>%
bind_cols(predict(final_fit, .)) %>%
conf_mat(truth = Y, estimate = .pred_class)
# test
test_set %>%
bind_cols(predict(final_fit, new_data = .)) %>%
my_metric(truth = Y, estimate = .pred_class)
test_set %>%
bind_cols(predict(final_fit, .)) %>%
conf_mat(truth = Y, estimate = .pred_class)
# Prediksi ----------------------------------------------------------------
preds <- predict(final_fit, new_data = df_test) %>% pull()
preds
# Submission --------------------------------------------------------------
hasil <- df_test %>%
select(ID) %>%
mutate(Y = preds)
model_res %>%
autoplot()
preds <- df_test %>%
mutate(
p = case_when(
ID %in% id1 ~ 1,
ID %in% id2 ~ 2,
TRUE ~ 0
)
# p = ifelse(ID %in% id1, 1, NA),
# p = ifelse(is.na(p) & (ID %in% id2), 2, pred)
) %>%
pull(p)
sum(preds == hasil$Y)
sum(preds != hasil$Y)
1- 1/16
n <- 500
ybar <- 50
s2 <- 2
# sample from the joint posterior (mu, tau | data)
mu <- rep(NA, 11000)
tau <- rep(NA, 11000)
T <- 1000 # burnin
burnin <- 1000 # burnin
tau[1] <- 1 # initialisation
for(i in 2:11000) {
mu[i] <- rnorm(n = 1, mean = ybar,
sd = sqrt(1/(n*tau[i-1])))
tau[i] <- rgamma(n = 1, shape = n/2,
scale = 2/((n-1)*s2 + n*(mu[i]-ybar)^2))
}
mu <- mu[-(1:burnin)] # remove burnin
tau <- tau[-(1:burnin)] # remove burnin
#Plot distribusi posterior
hist(mu, prob=TRUE,main="Distribusi Parameter Mu")
lines(density(mu), col="red")
hist(tau, prob=TRUE,main="Distribusi Parameter tau")
lines(density(tau), col="red")
dunif(1, min=0, max=10, log = T)
dunif(5, min=0, max=10, log = T)
dunif(5, min=0, max=10, log = F)
dunif(1, min=0, max=10, log = F)
dunif(5, min=0, max=10, log = F)
dunif(9, min=0, max=10, log = F)
dunif(9)
dunif(0.1, min=0, max=10, log = F)
dunif(0.4, min=0, max=10, log = F)
dunif(11, min=0, max=10, log = F)
dunif(9, min=0, max=10, log = F)
library(nnet)
library(neuralnet)
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
my_recipe <- recipe(Y ~ ., df_train[-1]) %>%
step_dummy(all_nominal_predictors()) %>%
step_impute_knn(X1) %>%
step_scale(X2) %>%
step_mutate(xxx = X1 * X2) %>%
prep()
loc <- 'D:/Downloads/data-mining'
list.files(path = loc)
df_train <- read.csv(file.path(loc, "training.csv"))
df_test <- read.csv(file.path(loc, "testing.csv"))
my_recipe <- recipe(Y ~ ., df_train[-1]) %>%
step_dummy(all_nominal_predictors()) %>%
step_impute_knn(X1) %>%
step_scale(X2) %>%
step_mutate(xxx = X1 * X2) %>%
prep()
dat1 <- bake(my_recipe, df_train[-1])
head(dat1)
my_recipe <- recipe(Y ~ ., df_train[-1]) %>%
step_dummy(all_nominal_predictors()) %>%
prep()
dat1 <- bake(my_recipe, df_train[-1])
head(dat1)
dat1
my_recipe <- recipe(Y ~ ., df_train) %>%
step_dummy(all_nominal_predictors()) %>%
prep()
dat1 <- bake(my_recipe, df_train[-1])
head(dat1)
dat1 <- bake(my_recipe, df_train)
head(dat1)
mlp_model2 <- neuralnet(Y ~ ID, data = dat1, hidden = c(8, 4))
# Model -------------------------------------------------------------------
mlp_model <- nnet(Y ~ ., data = dat, size = 5)
# Model -------------------------------------------------------------------
mlp_model <- nnet(Y ~ ., data = dat1, size = 5)
pred <- predict(mlp_model, df_train, type = 'class')
# Model -------------------------------------------------------------------
mlp_model <- nnet(Y ~ ., data = dat1, size = 5)
pred <- predict(mlp_model, df_train, type = 'class')
mlp_model
# Model -------------------------------------------------------------------
mlp_model <- nnet(Y ~ ., data = dat1[-1], size = 5)
pred <- predict(mlp_model, df_train, type = 'class')
cm <- table(actual = df_train$Y, prediction = pred)
pred <- predict(mlp_model, dat1, type = 'class')
# Model -------------------------------------------------------------------
mlp_model <- nnet(Y ~ ., data = dat1[-1], size = 5)
pred <- predict(mlp_model, dat1, type = 'class')
# Model -------------------------------------------------------------------
mlp_model <- nnet(Y ~ ., data = dat1[-1], size = 10)
pred <- predict(mlp_model, dat1[-1], type = 'class')
dat1[-1]
pred <- predict(mlp_model, dat1[-1], type = 'class')
cm <- table(actual = df_train$Y, prediction = pred)
cm
sum(diag(cm))/sum(cm)
# Model -------------------------------------------------------------------
mlp_model <- nnet(Y ~ ., data = df_train[-1], size = 10)
pred <- predict(mlp_model, df_train[-1], type = 'class')
cm <- table(actual = df_train$Y, prediction = pred)
library(nnet)
library(neuralnet)
# Model -------------------------------------------------------------------
mlp_model <- nnet(Y ~ ., data = df_train[-1], size = 10)
pred <- predict(mlp_model, df_train[-1], type = 'class')
cm <- table(actual = df_train$Y, prediction = pred)
# Model -------------------------------------------------------------------
mlp_model <- nnet::nnet(Y ~ ., data = df_train[-1], size = 10)
pred <- predict(mlp_model, df_train[-1], type = 'class')
pred <- predict(mlp_model, df_train[-1])
pred
cm <- table(actual = df_train$Y, prediction = pred[1])
cm
cm <- table(actual = df_train$Y, prediction = pred[,1])
cm
loc <- 'D:/Downloads/data-mining'
list.files(path = loc)
df_train <- read.csv(file.path(loc, "training.csv"))
df_test <- read.csv(file.path(loc, "testing.csv"))
df_train %>%
mutate(Y = as.factor(Y)) %>%
mutate_at(vars(X1:X4, X7, X9:X16), ~ as.factor(.x))
df_test <- df_test %>%
mutate_at(vars(X1:X4, X7, X9:X16), ~ as.factor(.x))
# Model -------------------------------------------------------------------
mlp_model <- nnet::nnet(Y ~ ., data = df_train[-1], size = 10)
pred <- predict(mlp_model, df_train[-1])
pred
cm <- table(actual = df_train$Y, prediction = pred[,1])
cm
pred <- predict(mlp_model, df_train[-1], type = 'class')
pred
df_train <- df_train %>%
mutate(Y = as.factor(Y)) %>%
mutate_at(vars(X1:X4, X7, X9:X16), ~ as.factor(.x))
df_test <- df_test %>%
mutate_at(vars(X1:X4, X7, X9:X16), ~ as.factor(.x))
# Model -------------------------------------------------------------------
mlp_model <- nnet::nnet(Y ~ ., data = df_train[-1], size = 10)
pred <- predict(mlp_model, df_train[-1], type = 'class')
pred
cm <- table(actual = df_train$Y, prediction = pred[,1])
cm
pred
cm <- table(actual = df_train$Y, prediction = pred)
cm
sum(diag(cm))/sum(cm)
my_recipe <- recipe(Y ~ ., df_train) %>%
step_dummy(all_nominal_predictors()) %>%
prep()
dat1 <- bake(my_recipe, df_train)
head(dat1)
mlp_model2 <- nnet::nnet(Y ~ ., data = dat1[-1], size = 10)
pred <- predict(mlp_model, dat1[-1], type = 'class')
dat1
pred <- predict(mlp_model, dat1[-1], type = 'class')
pred
pred <- predict(mlp_model, dat1[-1])
pred <- predict(mlp_model2, dat1[-1])
pred
pred <- predict(mlp_model2, dat1[-1], type = 'clas')
pred
cm2 <- table(actual = df_train$Y, prediction = pred)
cm2
sum(diag(cm2))/sum(cm2)
mlp_model2 <- nnet::nnet(Y ~ ., data = dat1[-1], size = 32)
pred <- predict(mlp_model2, dat1[-1], type = 'class')
pred2 <- predict(mlp_model2, dat1[-1], type = 'class')
pred2
cm2 <- table(actual = df_train$Y, prediction = pred)
cm2
sum(diag(cm2))/sum(cm2)
mlp_model2 <- nnet::nnet(Y ~ ID, data = dat1, size = 32)
pred2 <- predict(mlp_model2, dat1[-1], type = 'class')
cm2 <- table(actual = df_train$Y, prediction = pred)
pred2 <- predict(mlp_model2, dat1, type = 'class')
cm2 <- table(actual = df_train$Y, prediction = pred)
cm2
sum(diag(cm2))/sum(cm2)
nn <- neuralnet(Y ~ ., data = dat1[-1], hidden = c(8, 4))
?neuralnet
nn <- neuralnet(Y ~ ., data = dat1[-1], hidden = c(8, 4))
sample(1:10, 3)
sample(1:10, 3)
sample(1:10, 3)
sample(1:10, 3)
sample(1:10, 3)
set.seed(1)
sample(1:10, 3)
set.seed(1)
sample(1:10, 3)
set.seed(1)
sample(1:10, 3)
