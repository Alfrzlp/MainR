information_gain <- function(y, x, func = minimize_fun){
# entp_tot <- func(y)
s <- length(y)
s1 <- length(y[x])
s2 <- s - s1
# return value ig
if (s1 == 0 | s2 == 0) return(0)
else return(func(y) - s1/s * func(y[x]) - s2/s * func(y[!x]))
}
# Max Information Gain For A Single Feature
get_max_ig <- function(y, x, func = minimize_fun, min_n = min_n){
best_ig <- best_split <- previous_split <-  NA
is_numeric <- !(is.factor(x)|is.logical(x)|is.character(x))
for (val in sort(unique(x))) {
# jika regresi
if (is_numeric) mask <- x < val
# jika klasifikasi
else mask <- x == val
ig <- information_gain(y, mask, func)
s1 <- sum(mask)
s2 <- length(mask) - s1
if(s1 >= min_n & s2 >= min_n){
if(is.na(best_ig)){
best_ig <- ig
best_split <- ifelse(is.na(previous_split), val, mean(c(val, previous_split)))
} else if(ig > best_ig){
best_ig <- ig
best_split <- ifelse(is_numeric, mean(c(val, previous_split)), val)
}
}
previous_split <- val
}
return(
list(
"best_ig" = best_ig,
"best_split" = best_split,
"is_numeric" = is_numeric
)
)
}
get_feature_split <- function(y, x_pred, ...){
res <- sapply(x_pred, function(x) get_max_ig(y, x, ...))
idx <- which.max(res['best_ig', ])
if(length(idx) == 0) return(res)
else return(as.data.frame(c(res[, idx], var = names(idx))))
}
get_idsplit <- function(dat, res_ig){
if(res_ig$is_numeric) idx <- dat[, res_ig$var] < res_ig$best_split
else idx <- dat[, res_ig$var] == res_ig$best_split
return(idx)
}
generate <- function(dat, depth = 0){
y <- dat[, 1]
x <- dat[, -1]
is_regression <- identical(minimize_fun, variance)
mode_y <- names(which.max(table(y)))
output <- list(depth = depth)
if(!is_regression & length(unique(y)) == 1){
output$pos <- 'leaf'
output$pred <- mode_y
return(output)
}
if(depth <= max_depth){
res <- get_feature_split(y, x, func = minimize_fun, min_n = min_n)
if(all(is.na(res[1, ]))){
output$pos <- 'leaf'
output$pred <- ifelse(is_regression, mean(y), mode_y)
return(output)
} else {
idx <- get_idsplit(dat, res)
if(res$best_ig > min_ig){
if(sum(idx) >= min_n & length(idx) - sum(idx) >= min_n){
if(output$depth + 1 > max_depth){
output$pos <- 'leaf'
output$pred <- ifelse(is_regression, mean(y), mode_y)
return(output)
} else {
output$pos <- ifelse(depth == 0, 'root', 'decision')
output$variabel <- res$var
output$is_numeric <- res$is_numeric
output$split <- res$best_split
output$right <- generate(dat[idx, ], depth = output$depth + 1)
output$left <- generate(dat[!idx, ], depth = output$depth + 1)
return(output)
}
}
} else {
output$pos <- 'leaf'
output$pred <- ifelse(is_regression, mean(y), mode_y)
return(output)
}
}
}
}
to_print <- list()
dat <- model.frame(formula, dat)
return(invisible(generate(dat)))
}
hasil3 <- decision_tree(
formula = Kyphosis~., data = kyphosis,
minimize_fun = 'gini_impurity',
max_depth = 3
)
kyphosis
hasil3 <- decision_tree(
Kyphosis~., data = kyphosis,
minimize_fun = 'gini_impurity',
max_depth = 3
)
decision_tree <- function(formula, data, max_depth = 3, min_n = 20,  min_ig = 1e-3, minimize_fun = 'gini_impurity') {
gini_impurity <- function(y){
if(length(y) == 0) return(0)
p <- table(y) / length(y)
1 - sum(p^2)
}
variance <- function(y){
if(length(y) <= 1) return(0)
var(y)
}
entropy <- function(y) {
p <- table(y) / length(y)
-sum(p * log2(p + 1e-9))
}
if(minimize_fun == 'gini_impurity') minimize_fun <- gini_impurity
else if(minimize_fun == 'variance') minimize_fun <- variance
else if(minimize_fun == 'entropy') minimize_fun <- entropy
information_gain <- function(y, x, func = minimize_fun){
# entp_tot <- func(y)
s <- length(y)
s1 <- length(y[x])
s2 <- s - s1
# return value ig
if (s1 == 0 | s2 == 0) return(0)
else return(func(y) - s1/s * func(y[x]) - s2/s * func(y[!x]))
}
# Max Information Gain For A Single Feature
get_max_ig <- function(y, x, func = minimize_fun, min_n = min_n){
best_ig <- best_split <- previous_split <-  NA
is_numeric <- !(is.factor(x)|is.logical(x)|is.character(x))
for (val in sort(unique(x))) {
# jika regresi
if (is_numeric) mask <- x < val
# jika klasifikasi
else mask <- x == val
ig <- information_gain(y, mask, func)
s1 <- sum(mask)
s2 <- length(mask) - s1
if(s1 >= min_n & s2 >= min_n){
if(is.na(best_ig)){
best_ig <- ig
best_split <- ifelse(is.na(previous_split), val, mean(c(val, previous_split)))
} else if(ig > best_ig){
best_ig <- ig
best_split <- ifelse(is_numeric, mean(c(val, previous_split)), val)
}
}
previous_split <- val
}
return(
list(
"best_ig" = best_ig,
"best_split" = best_split,
"is_numeric" = is_numeric
)
)
}
get_feature_split <- function(y, x_pred, ...){
res <- sapply(x_pred, function(x) get_max_ig(y, x, ...))
idx <- which.max(res['best_ig', ])
if(length(idx) == 0) return(res)
else return(as.data.frame(c(res[, idx], var = names(idx))))
}
get_idsplit <- function(dat, res_ig){
if(res_ig$is_numeric) idx <- dat[, res_ig$var] < res_ig$best_split
else idx <- dat[, res_ig$var] == res_ig$best_split
return(idx)
}
generate <- function(dat, depth = 0){
y <- dat[, 1]
x <- dat[, -1]
is_regression <- identical(minimize_fun, variance)
mode_y <- names(which.max(table(y)))
output <- list(depth = depth)
if(!is_regression & length(unique(y)) == 1){
output$pos <- 'leaf'
output$pred <- mode_y
return(output)
}
if(depth <= max_depth){
res <- get_feature_split(y, x, func = minimize_fun, min_n = min_n)
if(all(is.na(res[1, ]))){
output$pos <- 'leaf'
output$pred <- ifelse(is_regression, mean(y), mode_y)
return(output)
} else {
idx <- get_idsplit(dat, res)
if(res$best_ig > min_ig){
if(sum(idx) >= min_n & length(idx) - sum(idx) >= min_n){
if(output$depth + 1 > max_depth){
output$pos <- 'leaf'
output$pred <- ifelse(is_regression, mean(y), mode_y)
return(output)
} else {
output$pos <- ifelse(depth == 0, 'root', 'decision')
output$variabel <- res$var
output$is_numeric <- res$is_numeric
output$split <- res$best_split
output$right <- generate(dat[idx, ], depth = output$depth + 1)
output$left <- generate(dat[!idx, ], depth = output$depth + 1)
return(output)
}
}
} else {
output$pos <- 'leaf'
output$pred <- ifelse(is_regression, mean(y), mode_y)
return(output)
}
}
}
}
dat <- model.frame(formula, data)
return(invisible(generate(dat)))
}
hasil3 <- decision_tree(
Kyphosis~., data = kyphosis,
minimize_fun = 'gini_impurity',
max_depth = 3
)
print_res <- function(res){
data.tree::FromListSimple(.change_name(res))
}
print_res(hasil3)
tree::tree(
Kyphosis~., data = kyphosis,
minsize = 40, mincut = 20, mindev = 1e-7
)
citation('tree')
citation('rpart')
rpart::rpart(
Kyphosis~., data = kyphosis,
maxdepth = 3, minbucket = 20,
minsplit = 40, cp = 1e-9
)
?tree::tree
?tree::tree.control
# minsize = The smallest allowed node size: a weighted quantity. The default is 10
tree::tree(
Kyphosis~., data = kyphosis,
minsize = 10, mincut = 20, mindev = 1e-7
)
styler::tidyverse_style()
styler:::style_active_file()
styler:::style_active_file()
DecisionTree <- setRefClass(
"DecisionTree",
fields = list(
minimize_func = "function",
min_information_gain = "numeric",
# min leaf size
min_n = "numeric",
max_depth = "numeric"
),
methods = list(
initialize = function(...) {},
fit = function(y, x_pred) {},
predict = function(x_pred) {}
)
)
DecisionTree <- setRefClass(
"DecisionTree",
fields = list(
formula = "formula",
data = "data.frame",
minimize_func = "function",
min_information_gain = "numeric",
# min leaf size
min_n = "numeric",
max_depth = "numeric"
),
methods = list(
initialize = function(...) {},
fit = function(y, x_pred) {},
predict = function(x_pred) {}
)
)
DecisionTree
DecisionTree <- setRefClass(
"DecisionTree",
fields = list(
formula = "formula",
data = "data.frame",
max_depth = "numeric",
# min leaf size
min_n = "numeric",
minimize_func = "function",
min_information_gain = "numeric"
),
methods = list(
initialize = function(...) {},
fit = function(y, x_pred) {},
predict = function(x_pred) {}
)
)
decision_tree <- function(formula, data, max_depth = 3, min_n = 20,  min_ig = 1e-3, minimize_fun = 'gini_impurity') {
gini_impurity <- function(y){
if(length(y) == 0) return(0)
p <- table(y) / length(y)
1 - sum(p^2)
}
variance <- function(y){
if(length(y) <= 1) return(0)
var(y)
}
entropy <- function(y) {
p <- table(y) / length(y)
-sum(p * log2(p + 1e-9))
}
if(minimize_fun == 'gini_impurity') minimize_fun <- gini_impurity
else if(minimize_fun == 'variance') minimize_fun <- variance
else if(minimize_fun == 'entropy') minimize_fun <- entropy
information_gain <- function(y, x, func = minimize_fun){
# entp_tot <- func(y)
s <- length(y)
s1 <- length(y[x])
s2 <- s - s1
# return value ig
if (s1 == 0 | s2 == 0) return(0)
else return(func(y) - s1/s * func(y[x]) - s2/s * func(y[!x]))
}
# Max Information Gain For A Single Feature
get_max_ig <- function(y, x, func = minimize_fun, min_n = min_n){
best_ig <- best_split <- previous_split <-  NA
is_numeric <- !(is.factor(x)|is.logical(x)|is.character(x))
for (val in sort(unique(x))) {
# jika regresi
if (is_numeric) mask <- x < val
# jika klasifikasi
else mask <- x == val
ig <- information_gain(y, mask, func)
s1 <- sum(mask)
s2 <- length(mask) - s1
if(s1 >= min_n & s2 >= min_n){
if(is.na(best_ig)){
best_ig <- ig
best_split <- ifelse(is.na(previous_split), val, mean(c(val, previous_split)))
} else if(ig > best_ig){
best_ig <- ig
best_split <- ifelse(is_numeric, mean(c(val, previous_split)), val)
}
}
previous_split <- val
}
return(
list(
"best_ig" = best_ig,
"best_split" = best_split,
"is_numeric" = is_numeric
)
)
}
get_feature_split <- function(y, x_pred, ...){
res <- sapply(x_pred, function(x) get_max_ig(y, x, ...))
idx <- which.max(res['best_ig', ])
if(length(idx) == 0) return(res)
else return(as.data.frame(c(res[, idx], var = names(idx))))
}
get_idsplit <- function(dat, res_ig){
if(res_ig$is_numeric) idx <- dat[, res_ig$var] < res_ig$best_split
else idx <- dat[, res_ig$var] == res_ig$best_split
return(idx)
}
generate <- function(dat, depth = 0){
y <- dat[, 1]
x <- dat[, -1]
is_regression <- identical(minimize_fun, variance)
mode_y <- names(which.max(table(y)))
output <- list(depth = depth)
if(!is_regression & length(unique(y)) == 1){
output$pos <- 'leaf'
output$pred <- mode_y
return(output)
}
if(depth <= max_depth){
res <- get_feature_split(y, x, func = minimize_fun, min_n = min_n)
if(all(is.na(res[1, ]))){
output$pos <- 'leaf'
output$pred <- ifelse(is_regression, mean(y), mode_y)
return(output)
} else {
idx <- get_idsplit(dat, res)
if(res$best_ig > min_ig){
if(sum(idx) >= min_n & length(idx) - sum(idx) >= min_n){
if(output$depth + 1 > max_depth){
output$pos <- 'leaf'
output$pred <- ifelse(is_regression, mean(y), mode_y)
return(output)
} else {
output$pos <- ifelse(depth == 0, 'root', 'decision')
output$variabel <- res$var
output$is_numeric <- res$is_numeric
output$split <- res$best_split
output$right <- generate(dat[idx, ], depth = output$depth + 1)
output$left <- generate(dat[!idx, ], depth = output$depth + 1)
return(output)
}
}
} else {
output$pos <- 'leaf'
output$pred <- ifelse(is_regression, mean(y), mode_y)
return(output)
}
}
}
}
dat <- model.frame(formula, data)
result <- generate(dat)
class(result) <- 'DecisionTree'
return(invisible(result))
}
hasil <- decision_tree(
species ~ ., data = iris,
minimize_fun = 'gini_impurity',
max_depth = 3,
min_n = 20,
min_ig = 1e-3
)
str(hasil)
predict.DecisionTree <- function(model){
res <- modify(split(x, 1:nrow(x)), .f = .pred, result = hasil)
unlist(res, use.names = F)
}
predict(hasil)
predict.DecisionTree <- function(model, newdata){
res <- modify(split(newdata, 1:nrow(newdata)), .f = .pred, result = model)
unlist(res, use.names = F)
}
predict(hasil, newdata = iris[, -5])
# S3 ----------------------------------------------------------------------
summary.DecisionTree <- function(model){
data.tree::FromListSimple(.change_name(res))
}
print.DecisionTree <- function(model){
data.tree::FromListSimple(.change_name(res))
}
print(hasil)
# S3 ----------------------------------------------------------------------
summary.DecisionTree <- function(model){
data.tree::FromListSimple(.change_name(model))
}
print.DecisionTree <- function(model){
data.tree::FromListSimple(.change_name(model))
}
print(hasil)
summary(hasil)
hasil
apply(iris[, -5], 2, function(val) .pred(val, result = hasil))
# helper ------------------------------------------------------------------
.change_name <- function(result){
idx_left <- which.max(names(result) == "left")
idx_right <- which.max(names(result) == "right")
if(length(idx_left) == 0) return(result)
else{
names(result)[names(result) == "right"] <- str_glue(' {result$variabel} {ifelse(result$is_numeric, "<", "=")} {result$split} {ifelse(result$right$pos == "leaf", paste("-- Predict :",result$right$pred), "")}')
names(result)[names(result) == "left"] <- str_glue(' {result$variabel} {ifelse(result$is_numeric, "≥", "!=")} {result$split} {ifelse(result$left$pos == "leaf", paste("-- Predict :",result$left$pred), "")}')
result[[idx_left]] <- .change_name(result[[idx_left]])
result[[idx_right]] <- .change_name(result[[idx_right]])
return(result)
}
}
styler:::style_active_file()
.change_name <- function(result) {
idx_left <- which.max(names(result) == "left")
idx_right <- which.max(names(result) == "right")
if (length(idx_left) == 0) {
return(result)
} else {
names(result)[names(result) == "right"] <- str_glue(' {result$variabel} {ifelse(result$is_numeric, "<", "=")} {result$split} {ifelse(result$right$pos == "leaf", paste("-- Predict :",result$right$pred), "")}')
names(result)[names(result) == "left"] <- str_glue(' {result$variabel} {ifelse(result$is_numeric, "≥", "!=")} {result$split} {ifelse(result$left$pos == "leaf", paste("-- Predict :",result$left$pred), "")}')
result[[idx_left]] <- .change_name(result[[idx_left]])
result[[idx_right]] <- .change_name(result[[idx_right]])
return(result)
}
}
.pred <- function(x, result) {
if (result$pos == "leaf") {
return(result$pred)
} else {
if (result$is_numeric) {
if (x[, result$variabel] < result$split) {
.pred(x, result$right)
} else {
.pred(x, result$left)
}
} else {
if (x[, result$variabel] == result$split) {
.pred(x, result$right)
} else {
.pred(x, result$left)
}
}
}
}
print(hasil)
data.tree::FromListSimple(.change_name(hasil), nodeName = 'aa')
data.tree::FromListSimple(.change_name(hasil), nodeName = 'aa', nameName = 'aa')
data.tree::FromListSimple(.change_name(hasil), levelName = '')
data.tree::FromListSimple(.change_name(hasil)) %>% class()
apply(iris[, -5], 2, function(val) .pred(val, result = hasil))
apply(iris[, -5], 1, function(val) .pred(val, result = hasil))
dat
as.list(dat)
as.list(t(dat))
